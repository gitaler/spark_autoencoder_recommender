{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ba0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb929f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9596102",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342ad010",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1950\n",
    "end_year = 1960\n",
    "num_workers = 8\n",
    "collect_stats = True\n",
    "executable = 'python.exe'\n",
    "monitor = 'resources_monitor.py'\n",
    "\n",
    "setup = f'_{str(end_year)[-2:]}_{num_workers}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3033322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created folder data_loading\n",
      "created folder ratings_prep\n",
      "created folder movies_prep\n",
      "created folder movies_ratings_join\n",
      "created folder subsets_stats\n",
      "created folder ratings_array\n",
      "created folder model_train_eval\n"
     ]
    }
   ],
   "source": [
    "# creating stats folders\n",
    "folders = ['data_loading','ratings_prep','movies_prep','movies_ratings_join','subsets_stats','ratings_array','model_train_eval']\n",
    "folder_path = \"movie_lens_stats\"\n",
    "\n",
    "if not os.path.exists(folder_path): os.mkdir(folder_path)\n",
    "for f in folders: \n",
    "    if os.path.exists(folder_path+f'/{f}'): continue\n",
    "    os.mkdir(folder_path+f'/{f}')\n",
    "    print(f'created folder {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20467f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('AE_Rec_Sys') \\\n",
    "        .setMaster('local[*]') \\\n",
    "        .set(\"spark.driver.memory\", \"9g\") \\\n",
    "        .set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "\n",
    "conf.set(\"spark.executor.instances\", str(num_workers))\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48675187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-I65QHVSM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AE_Rec_Sys</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=AE_Rec_Sys>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b963b",
   "metadata": {},
   "source": [
    "### Step 0: data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b831d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "dest = f'./{folder_path}/{folders[step]}/{folders[step]}{setup}.csv'\n",
    "if collect_stats: stats_collector = subprocess.Popen([executable, monitor, dest, '1'])\n",
    "    \n",
    "ratings = sc.textFile('ml-25m/ratings.csv')\n",
    "\n",
    "if collect_stats: \n",
    "    action = ratings.count()\n",
    "    stats_collector.kill()\n",
    "if num_workers == 1: ratings = ratings.repartition(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f77fd9",
   "metadata": {},
   "source": [
    "### Step 1: ratings data cleaning and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601fd386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('296', ('1', 5.0)),\n",
       " ('306', ('1', 3.5)),\n",
       " ('307', ('1', 5.0)),\n",
       " ('665', ('1', 5.0)),\n",
       " ('899', ('1', 3.5)),\n",
       " ('1088', ('1', 4.0)),\n",
       " ('1175', ('1', 3.5)),\n",
       " ('1217', ('1', 3.5)),\n",
       " ('1237', ('1', 5.0)),\n",
       " ('1250', ('1', 4.0))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET (movieId, (userId, rating)) ratings RDD\n",
    "\n",
    "step = 1\n",
    "dest = f'./{folder_path}/{folders[step]}/{folders[step]}{setup}.csv'\n",
    "if collect_stats: stats_collector = subprocess.Popen([executable, monitor, dest, '1'])\n",
    "    \n",
    "ratings = ratings.filter(lambda line: line != 'userId,movieId,rating,timestamp') \\\n",
    "            .map(lambda line: line.split(',')) \\\n",
    "            .map(lambda rating: (rating[1], (rating[0], float(rating[2]))))\n",
    "\n",
    "if collect_stats: \n",
    "    action = ratings.count()\n",
    "    stats_collector.kill()\n",
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e2963",
   "metadata": {},
   "source": [
    "### STEP 2: movies data cleaning, treatment and filtering by year range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b73260",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = sc.textFile('ml-25m/movies.csv')\n",
    "if num_workers == 1: movies = movies.repartition(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c03102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('659', ('Purple Noon (Plein soleil)', 1960)),\n",
       " ('668', ('Song of the Little Road (Pather Panchali)', 1955)),\n",
       " ('670', ('World of Apu, The (Apur Sansar)', 1959)),\n",
       " ('755', ('Kim', 1950)),\n",
       " ('820', ('Death in the Garden (Mort en ce jardin, La)', 1956)),\n",
       " ('841', ('Eyes Without a Face (Yeux sans visage, Les)', 1959)),\n",
       " ('854', ('Ballad of Narayama, The (Narayama Bushiko)', 1958)),\n",
       " ('899', (\"Singin' in the Rain\", 1952)),\n",
       " ('900', ('American in Paris, An', 1951)),\n",
       " ('901', ('Funny Face', 1957))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET (movie_id, (title, year)) movies RDD\n",
    "# remove 'movieId,title,genres' header line and all films without a specified year\n",
    "# movie title may contain ','\n",
    "# movie titles containing ',' are enclosed in double quotes\n",
    "\n",
    "import re\n",
    "pattern = r'\\(\\d{4}\\)' # '(yyyy)' year format\n",
    "\n",
    "def remove_enclosing_double_quotes(movie_string):\n",
    "    if movie_string[0] == '\"': movie_string = movie_string[1:]\n",
    "    if movie_string[-1] == '\"': movie_string = movie_string[:-1]\n",
    "    return movie_string\n",
    "\n",
    "step = 2\n",
    "dest = f'./{folder_path}/{folders[step]}/{folders[step]}{setup}.csv'\n",
    "if collect_stats: stats_collector = subprocess.Popen([executable, monitor, dest, 'no_interval'])\n",
    "    \n",
    "movies = movies.filter(lambda line: re.search(pattern, line)) \\\n",
    "            .map(lambda line: line.split(',')) \\\n",
    "            .map(lambda movie: (movie[0], ','.join(movie[1:-1]))) \\\n",
    "            .map(lambda movie: (movie[0], remove_enclosing_double_quotes(movie[1]))) \\\n",
    "            .map(lambda movie: (movie[0], movie[1][:-7], re.findall(pattern, movie[1])[-1])) \\\n",
    "            .map(lambda movie: (movie[0], movie[1], movie[2].translate(str.maketrans('', '', '()')))) \\\n",
    "            .map(lambda movie: (movie[0], (movie[1], int(movie[2])))) \\\n",
    "            .filter(lambda movie: movie[1][1] >= start_year and movie[1][1] <= end_year)\n",
    "\n",
    "# (movie_id, (title, year))\n",
    "if collect_stats: \n",
    "    action = movies.count()\n",
    "    stats_collector.kill()\n",
    "movies.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2009a5",
   "metadata": {},
   "source": [
    "### STEP 3: JOIN ratings and movies RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9812e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1250', 4.0),\n",
       " ('9', '1250', 5.0),\n",
       " ('20', '1250', 4.5),\n",
       " ('38', '1250', 3.5),\n",
       " ('58', '1250', 4.5),\n",
       " ('59', '1250', 4.0),\n",
       " ('72', '1250', 4.0),\n",
       " ('75', '1250', 3.5),\n",
       " ('86', '1250', 5.0),\n",
       " ('99', '1250', 3.5)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joined rdds results in (movie_id, ((userId, rating), (title, year)))\n",
    "\n",
    "step = 3\n",
    "dest = f'./{folder_path}/{folders[step]}/{folders[step]}{setup}.csv'\n",
    "if collect_stats: stats_collector = subprocess.Popen([executable, monitor, dest, '1'])\n",
    "    \n",
    "# filtered_ratings\n",
    "ratings = ratings.join(movies) \\\n",
    "            .map(lambda rating: (rating[1][0][0], rating[0], rating[1][0][1]))\n",
    "\n",
    "if collect_stats: \n",
    "    action = ratings.count()\n",
    "    stats_collector.kill()\n",
    "    \n",
    "# (userId, movie_id, rating)\n",
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18cb67a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this case, join produces more partitions\n",
    "if num_workers == 1: ratings = ratings.repartition(1)\n",
    "ratings.persist() # rdd.persist() since this rdd will be used different times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4dd935",
   "metadata": {},
   "source": [
    "### STEP 4: collecting subset stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3965d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of movies 2821\n",
      "number of users 77048\n",
      "number of ratings 565530\n"
     ]
    }
   ],
   "source": [
    "step = 4\n",
    "dest = f'./{folder_path}/{folders[step]}/{folders[step]}{setup}.csv'\n",
    "if collect_stats: stats_collector = subprocess.Popen([executable, monitor, dest, '1'])\n",
    "    \n",
    "# {'movie_id': rating_array_index}\n",
    "movies_ids = ratings.map(lambda x: x[1]).distinct()\n",
    "movies_id_index_pairs = {}\n",
    "for i, film in enumerate(movies_ids.collect()): \n",
    "    movies_id_index_pairs[film] = i\n",
    "movies_ids.unpersist()\n",
    "    \n",
    "# number of movies\n",
    "number_of_movies = len(movies_id_index_pairs)\n",
    "\n",
    "# max rating(useful for normalization)\n",
    "unique_ratings = ratings.map(lambda x: x[2]).distinct()\n",
    "max_rating = max(unique_ratings.collect())\n",
    "unique_ratings.unpersist()\n",
    "\n",
    "# number of users\n",
    "users_ids = ratings.map(lambda x: x[0]).distinct()\n",
    "number_of_users = users_ids.count()\n",
    "users_ids.unpersist()\n",
    "\n",
    "# number of ratings\n",
    "numer_of_ratings = ratings.count()\n",
    "\n",
    "if collect_stats: stats_collector.kill()\n",
    "\n",
    "# subset stats:\n",
    "print('number of movies', number_of_movies)\n",
    "print('number of users', number_of_users)\n",
    "print('number of ratings', numer_of_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9f8f6",
   "metadata": {},
   "source": [
    "### STEP 5: users' ratings arrays creation + normalization [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcab33f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0.6, 0. , 0. , ..., 0. , 0. , 0. ]),\n",
       " array([0.8, 0. , 0. , ..., 0. , 0. , 0. ]),\n",
       " array([0.6, 0. , 0. , ..., 0. , 0. , 0. ]),\n",
       " array([0.8, 0. , 0. , ..., 0. , 0. , 0. ]),\n",
       " array([0.8, 0. , 0. , ..., 0. , 0. , 0. ]),\n",
       " array([1., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0.7, 0. , 0. , ..., 0. , 0. , 0. ]),\n",
       " array([0.8, 0. , 0. , ..., 0. , 0. , 0. ]),\n",
       " array([0.8, 0. , 0. , ..., 0. , 0. , 0. ])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def full_ratings_array(ratings, num_films, films_idx):\n",
    "    ratings_array = np.zeros(num_films)\n",
    "    for rating in ratings: \n",
    "        ratings_array[films_idx[rating[0]]] = rating[1]\n",
    "    return ratings_array\n",
    "    \n",
    "step = 5\n",
    "dest = f'./{folder_path}/{folders[step]}/{folders[step]}{setup}.csv'\n",
    "if collect_stats: stats_collector = subprocess.Popen([executable, monitor, dest, 'no_interval'])\n",
    "\n",
    "aggregated_user_ratings = ratings.map(lambda x: (x[0], (x[1], x[2]))) \\\n",
    "                            .groupByKey() \\\n",
    "                            .map(lambda x : full_ratings_array(x[1], number_of_movies, movies_id_index_pairs)) \\\n",
    "                            .map(lambda rating: rating/max_rating)\n",
    "\n",
    "if collect_stats: \n",
    "    action = aggregated_user_ratings.count()\n",
    "    stats_collector.kill()\n",
    "    \n",
    "# (array_ratings[])\n",
    "aggregated_user_ratings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac9ec5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratings rdd is no longer needed. \n",
    "ratings.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa906c1",
   "metadata": {},
   "source": [
    "### autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc54480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_autoencoder(samples_dim):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=samples_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(150))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(samples_dim))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0386809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 150)               423300    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 150)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                3775      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 25)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 150)               3900      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 150)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2821)              425971    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2821)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 856,946\n",
      "Trainable params: 856,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = create_autoencoder(number_of_movies)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29a73bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Self-made epoch by epoch model training just to plot loss curves and show training behaviour. \n",
    "# # Elephas fit method does not provide losses history\n",
    "\n",
    "\n",
    "# from evaluable_training import evaluable_distributed_training\n",
    "# import numpy as np\n",
    "\n",
    "# train_losses, val_losses = evaluable_distributed_training(sc, aggregated_user_ratings, autoencoder, num_workers, 100)\n",
    "# np.save(f'train_losses{setup}.npy', np.array(train_losses))\n",
    "# np.save(f'val_losses{setup}.npy', np.array(val_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726c4aa",
   "metadata": {},
   "source": [
    "### STEP 6:  model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5185467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2733, 2642, 2707, 2723, 2692, 2697, 2621, 2653, 2703, 2750, 2662, 2755, 2650, 2601, 2588, 2722, 2624, 2710, 2678, 2571, 2611, 2577, 2821]\n"
     ]
    }
   ],
   "source": [
    "train_rdd, test_rdd = aggregated_user_ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# CHECK IF SAMPLES ARE WELL DISTRIBUTED OVER PARTITIONS\n",
    "partition_sizes = train_rdd.mapPartitions(lambda partition: [len(list(partition))])\n",
    "print(partition_sizes.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea9d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Synchronous training complete.\n"
     ]
    }
   ],
   "source": [
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "# (input, target) elephas rdd required format\n",
    "train_rdd = train_rdd.map(lambda rating_array: (rating_array,rating_array))\n",
    "epochs = 50\n",
    "spark_ae_model = SparkModel(autoencoder, frequency='epoch', mode='synchronous', num_workers=num_workers)\n",
    "\n",
    "step = 6\n",
    "dest = f'./{folder_path}/{folders[step]}/model_train{setup}.csv'\n",
    "if collect_stats: stats_collector = subprocess.Popen([executable, monitor, dest, '10'])\n",
    "\n",
    "spark_ae_model.fit(train_rdd, epochs=epochs, batch_size=64, verbose=1, validation_split=0.1)\n",
    "if collect_stats: stats_collector.kill()\n",
    "    \n",
    "# save the trained AE\n",
    "spark_ae_model.save(f'trained AEs/AE_model{setup}.keras')\n",
    "\n",
    "train_rdd = train_rdd.map(lambda rating_array: rating_array[0]) # removing redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa9b5b",
   "metadata": {},
   "source": [
    "### STEP 7: model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b5a9388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's performances on train set:\n",
      "\tmse:   0.0011734025231792842\n",
      "\trmse:  0.03425496348238141\n",
      "\n",
      "model's performances on test set:\n",
      "\tmse:   0.001184529329908802\n",
      "\trmse:  0.034416991877687424\n"
     ]
    }
   ],
   "source": [
    "from evaluable_training import calculate_avg_mse_loss\n",
    "import pandas as pd\n",
    "\n",
    "train_recs = spark_ae_model.predict(train_rdd)\n",
    "train_recs_rdd = sc.parallelize(train_recs)\n",
    "train_mse = calculate_avg_mse_loss(train_rdd, train_recs_rdd)\n",
    "train_recs_rdd.unpersist()\n",
    "\n",
    "test_recs = spark_ae_model.predict(test_rdd)\n",
    "test_recs_rdd = sc.parallelize(test_recs)\n",
    "test_mse = calculate_avg_mse_loss(test_rdd, test_recs_rdd)\n",
    "test_recs_rdd.unpersist()\n",
    "\n",
    "print('model\\'s performances on train set:')\n",
    "print('\\tmse:  ', train_mse)\n",
    "print('\\trmse: ', np.sqrt(train_mse))\n",
    "print()\n",
    "print('model\\'s performances on test set:')\n",
    "print('\\tmse:  ', test_mse)\n",
    "print('\\trmse: ', np.sqrt(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cabedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = {\n",
    "    'train_mse': [train_mse],\n",
    "    'train_rmse': [np.sqrt(train_mse)],\n",
    "    'test_mse': [test_mse],\n",
    "    'test_rmse': [np.sqrt(test_mse)],\n",
    "}\n",
    "df = pd.DataFrame(model_eval)\n",
    "dest = f'./{folder_path}/{folders[step]}/model_eval{setup}.csv'\n",
    "df.to_csv(dest, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e26dd8",
   "metadata": {},
   "source": [
    "### STEP 8: example of recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83454b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br><font size=\"3\">Suggested movie: </font><br><h1>Psycho (1960)</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RECOMMENDATION METHOD: Users with similar preferences will have similar representations in the latent space. \n",
    "# If two users are close in the latent space, it suggests that their preferences are alike. \n",
    "# Consequently, items that are well-rated by one user are likely to be well-rated by the other. \n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "AE = load_model(f'trained AEs/AE_model{setup}.keras')\n",
    "user_ratings = aggregated_user_ratings.takeSample(False, 1)[0]  # sampling a random user's ratings\n",
    "\n",
    "user_ratings = np.expand_dims(user_ratings, axis=0)\n",
    "rec = AE.predict(user_ratings, verbose=0)\n",
    "\n",
    "merged = np.stack([np.arange(0, number_of_movies), \n",
    "                    np.squeeze(user_ratings, axis=0), \n",
    "                    np.squeeze(rec, axis=0)\n",
    "                  ], axis=1)\n",
    "filtered_ratings = list(filter(lambda rating_pair: rating_pair[1] == 0, merged))\n",
    "recom_movie_index = max(filtered_ratings,key=lambda item: item[2])[0]\n",
    "movie_id = list(movies_id_index_pairs.keys())[int(recom_movie_index)]\n",
    "movie_data = movies.filter(lambda movie: movie[0]==movie_id).collect()[0]\n",
    "suggested_movie = f'{movie_data[1][0]} ({movie_data[1][1]})'\n",
    "\n",
    "display(HTML(f'<br><font size=\"3\">Suggested movie: </font><br><h1>{suggested_movie}</h1>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73737a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbcf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
