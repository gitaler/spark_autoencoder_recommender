{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ba0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb929f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9596102",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "342ad010",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1950\n",
    "end_year = 1960\n",
    "num_workers = 8\n",
    "setup = f'_{str(end_year)[-2:]}_{num_workers}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20467f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('AE_Rec_Sys') \\\n",
    "        .setMaster('local[*]') \\\n",
    "        .set(\"spark.driver.memory\", \"9g\") \\\n",
    "        .set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.driver.maxResultSize\", \"1g\")\n",
    "\n",
    "conf.set(\"spark.executor.instances\", str(num_workers))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48675187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-I65QHVSM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AE_Rec_Sys</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=AE_Rec_Sys>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b963b",
   "metadata": {},
   "source": [
    "### ratings data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601fd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET (movieId, (userId, rating)) ratings RDD\n",
    "\n",
    "ratings = sc.textFile('ml-25m/ratings.csv') \\\n",
    "            .filter(lambda line: line != 'userId,movieId,rating,timestamp') \\\n",
    "            .map(lambda line: line.split(',')) \\\n",
    "            .map(lambda rating: (rating[1], (rating[0], float(rating[2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f77fd9",
   "metadata": {},
   "source": [
    "### movies data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c03102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET (movie_id, (title, year)) movies RDD\n",
    "# remove 'movieId,title,genres' header line and all films without a specified year\n",
    "# movie title may contain ','\n",
    "# movie titles containing ',' are enclosed in double quotes\n",
    "\n",
    "import re\n",
    "pattern = r'\\(\\d{4}\\)' # '(yyyy)' year format\n",
    "\n",
    "def remove_enclosing_double_quotes(movie_string):\n",
    "    if movie_string[0] == '\"': movie_string = movie_string[1:]\n",
    "    if movie_string[-1] == '\"': movie_string = movie_string[:-1]\n",
    "    return movie_string\n",
    "\n",
    "movies = sc.textFile('ml-25m/movies.csv') \\\n",
    "            .filter(lambda line: re.search(pattern, line)) \\\n",
    "            .map(lambda line: line.split(',')) \\\n",
    "            .map(lambda movie: (movie[0], ','.join(movie[1:-1]))) \\\n",
    "            .map(lambda movie: (movie[0], remove_enclosing_double_quotes(movie[1]))) \\\n",
    "            .map(lambda movie: (movie[0], movie[1][:-7], re.findall(pattern, movie[1])[-1])) \\\n",
    "            .map(lambda movie: (movie[0], movie[1], movie[2].translate(str.maketrans('', '', '()')))) \\\n",
    "            .map(lambda movie: (movie[0], (movie[1], int(movie[2])))) \\\n",
    "            .filter(lambda movie: movie[1][1] >= start_year and movie[1][1] <= end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488b095",
   "metadata": {},
   "source": [
    "### data integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9812e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = ratings.join(movies) \\\n",
    "            .map(lambda rating: (rating[1][0][0], rating[0], rating[1][0][1]))\n",
    "\n",
    "ratings.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474273c",
   "metadata": {},
   "source": [
    "### statistical analysis of the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3965d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of movies 2821\n",
      "number of users 77048\n",
      "number of ratings 565530\n"
     ]
    }
   ],
   "source": [
    "# {'movie_id': rating_array_index}\n",
    "movies_ids = ratings.map(lambda x: x[1]).distinct()\n",
    "movies_id_index_pairs = {}\n",
    "for i, film in enumerate(movies_ids.collect()): \n",
    "    movies_id_index_pairs[film] = i\n",
    "movies_ids.unpersist()\n",
    "    \n",
    "# number of movies\n",
    "number_of_movies = len(movies_id_index_pairs)\n",
    "\n",
    "# max rating (useful for normalization)\n",
    "unique_ratings = ratings.map(lambda x: x[2]).distinct()\n",
    "max_rating = max(unique_ratings.collect())\n",
    "unique_ratings.unpersist()\n",
    "\n",
    "# number of users\n",
    "users_ids = ratings.map(lambda x: x[0]).distinct()\n",
    "number_of_users = users_ids.count()\n",
    "users_ids.unpersist()\n",
    "\n",
    "# number of ratings\n",
    "numer_of_ratings = ratings.count()\n",
    "\n",
    "# subset stats:\n",
    "print('number of movies', number_of_movies)\n",
    "print('number of users', number_of_users)\n",
    "print('number of ratings', numer_of_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76520c8e",
   "metadata": {},
   "source": [
    "### ratings arrays generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcab33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def full_ratings_array(ratings, num_films, films_idx):\n",
    "    ratings_array = np.zeros(num_films)\n",
    "    for rating in ratings: \n",
    "        ratings_array[films_idx[rating[0]]] = rating[1]\n",
    "    return ratings_array\n",
    "\n",
    "aggregated_user_ratings = ratings.map(lambda x: (x[0], (x[1], x[2]))) \\\n",
    "                            .groupByKey() \\\n",
    "                            .map(lambda x : full_ratings_array(x[1], number_of_movies, movies_id_index_pairs)) \\\n",
    "                            .map(lambda rating: rating/max_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eff14ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77048"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_user_ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0aee55",
   "metadata": {},
   "source": [
    "### autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc54480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_autoencoder(samples_dim):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=samples_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(150))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(samples_dim))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0386809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 150)               423300    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 150)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                3775      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 25)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 150)               3900      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 150)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2821)              425971    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2821)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 856,946\n",
      "Trainable params: 856,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = create_autoencoder(number_of_movies)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a73bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Self-made epoch by epoch model training just to plot loss curves and show training behaviour. \n",
    "# # Elephas fit method does not provide losses history\n",
    "\n",
    "# from evaluable_training import evaluable_distributed_training\n",
    "# import numpy as np\n",
    "\n",
    "# train_losses, val_losses = evaluable_distributed_training(sc, aggregated_user_ratings, autoencoder, num_workers, 100)\n",
    "# np.save(f'train_losses{setup}.npy', np.array(train_losses))\n",
    "# np.save(f'val_losses{setup}.npy', np.array(val_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726c4aa",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5185467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2733, 2642, 2707, 2723, 2692, 2697, 2621, 2653, 2703, 2750, 2662, 2755, 2650, 2601, 2588, 2722, 2624, 2710, 2678, 2571, 2611, 2577, 2821]\n"
     ]
    }
   ],
   "source": [
    "train_rdd, test_rdd = aggregated_user_ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# CHECK IF SAMPLES ARE WELL DISTRIBUTED OVER PARTITIONS\n",
    "partition_sizes = train_rdd.mapPartitions(lambda partition: [len(list(partition))])\n",
    "print(partition_sizes.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea9d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Synchronous training complete.\n"
     ]
    }
   ],
   "source": [
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "# (input, target) elephas rdd required format\n",
    "train_rdd = train_rdd.map(lambda ratings_array: (ratings_array,ratings_array))\n",
    "\n",
    "epochs = 50\n",
    "spark_ae_model = SparkModel(autoencoder, frequency='epoch', mode='synchronous', num_workers=num_workers)\n",
    "spark_ae_model.fit(train_rdd, epochs=epochs, batch_size=64, verbose=1, validation_split=0.1)\n",
    "spark_ae_model.save(f'trained AEs/AE_model{setup}.keras')\n",
    "\n",
    "train_rdd = train_rdd.map(lambda rating_array: rating_array[0]) # removing redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa9b5b",
   "metadata": {},
   "source": [
    "### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b5a9388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's performances on train set:\n",
      "\tmse:   0.0012489393410993225\n",
      "\trmse:  0.03534033589397988\n",
      "\n",
      "model's performances on test set:\n",
      "\tmse:   0.001259961455509893\n",
      "\trmse:  0.03549593576044859\n"
     ]
    }
   ],
   "source": [
    "from evaluable_training import calculate_avg_mse_loss\n",
    "import pandas as pd\n",
    "\n",
    "train_recs = spark_ae_model.predict(train_rdd)\n",
    "train_recs_rdd = sc.parallelize(train_recs)\n",
    "train_mse = calculate_avg_mse_loss(train_rdd, train_recs_rdd)\n",
    "train_recs_rdd.unpersist()\n",
    "\n",
    "test_recs = spark_ae_model.predict(test_rdd)\n",
    "test_recs_rdd = sc.parallelize(test_recs)\n",
    "test_mse = calculate_avg_mse_loss(test_rdd, test_recs_rdd)\n",
    "test_recs_rdd.unpersist()\n",
    "\n",
    "print('model\\'s performances on train set:')\n",
    "print('\\tmse:  ', train_mse)\n",
    "print('\\trmse: ', np.sqrt(train_mse))\n",
    "print()\n",
    "print('model\\'s performances on test set:')\n",
    "print('\\tmse:  ', test_mse)\n",
    "print('\\trmse: ', np.sqrt(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cabedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = {\n",
    "    'train_mse': [train_mse],\n",
    "    'train_rmse': [np.sqrt(train_mse)],\n",
    "    'test_mse': [test_mse],\n",
    "    'test_rmse': [np.sqrt(test_mse)],\n",
    "}\n",
    "df = pd.DataFrame(model_eval)\n",
    "dest = f'./{folder_path}/{folders[step]}/model_eval{setup}.csv'\n",
    "df.to_csv(dest, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e26dd8",
   "metadata": {},
   "source": [
    "### example of recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83454b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br><font size=\"3\">Suggested movie: </font><br><h1>Rear Window (1954)</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "AE = load_model(f'trained AEs/AE_model{setup}.keras')\n",
    "user_ratings = aggregated_user_ratings.takeSample(False, 1)[0]  # sampling a random user's ratings\n",
    "\n",
    "user_ratings = np.expand_dims(user_ratings, axis=0)\n",
    "rec = AE.predict(user_ratings, verbose=0)\n",
    "\n",
    "merged = np.stack([np.arange(0, number_of_movies), \n",
    "                    np.squeeze(user_ratings, axis=0), \n",
    "                    np.squeeze(rec, axis=0)\n",
    "                  ], axis=1)\n",
    "filtered_ratings = list(filter(lambda rating_pair: rating_pair[1] == 0, merged))\n",
    "recom_movie_index = max(filtered_ratings,key=lambda item: item[2])[0]\n",
    "movie_id = list(movies_id_index_pairs.keys())[int(recom_movie_index)]\n",
    "movie_data = movies.filter(lambda movie: movie[0]==movie_id).collect()[0]\n",
    "suggested_movie = f'{movie_data[1][0]} ({movie_data[1][1]})'\n",
    "\n",
    "# print('Suggested movie: ', movie_data[1][0], f'({movie_data[1][1]})')\n",
    "display(HTML(f'<br><font size=\"3\">Suggested movie: </font><br><h1>{suggested_movie}</h1>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73737a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbcf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
